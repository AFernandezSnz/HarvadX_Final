---
title: "HarvardX. Module 9: Data Science  \n   Final Project: E-commerce conversion prediction"
author: "Adelaida Fernández Sanz"
date: "9/02/2020"
output:
  pdf_document: default
  html_document: default
---
# Introduction and Aim of the Project

This project is part of the HarvardX Data Science Proffesional Certification: Capstone. 
The objective of the model is to be able to predict whether a user, defining user as a visitor to a certain e-commerce website, will make a purchase on the website or not.
It is about predicting the conversion per user of the e-commerce. For this, the data obtained in the realization of the article Sakar, C.O., Polat, S.O., Katircioglu, M., Neural Comput & Applic (2018) will be taken as a basis.
The database consists of vectorized variables that belong to 12,330 user sessions. Each session belongs to a different user obtained during a period of one year in order to avoid any tendency to a specific campaign, specific day, user profile or specific period.

# Method & Analysis

We will apply two different algoritms: SVM (Linear) and Random Forest since they are very efficient in predictive tasks that require regression and classification techniques.

Firstly, the database has been inspected to ensure that there is no missing value and to identify and inspect the variables of the database. According to the source cited above, the database consists of:

  • 10 numerical variables and 8 categorical variables.
  
  • The dependent variable, the one that we want to predict in the model is the variable "Revenue":the effectiveness in the purchase. It is a dichotomous variable: TRUE (If you buy), FALSE (do not buy).
  
  
The database is not balanced. The TRUE class is a minority (1908) compared to the FALSE class (10,422). So we will measure the performance of the model with the original database and with a balanced database to compare results and select the model with the best fit.We will use the downsampling technique in order to balance the data set.
Machine learning classifiers like SVM or Random Forest do not deal very well with unbalanced training datasets as they are sensitive to the proportions of the different classes. As a consequence, these algorithms tend to favor the class with the highest proportion of observations (known as the majority class), which can lead to biased accuracy metrics.

For the SVM approach, to achieve the best possible fit, under both models (unbalanced & balanced), we will look after the best C parameters:.
The C Parameter  (margin), represents the complexity constant. Specifies whether the model should be more generalized or more specific. The higher the value of the parameter, the greater the specificity, but this can lead to an overfitting. We will therefore test with 10 values in this range C = [0.01 : 0.2].
For the Random Forest approach we will look after the best mtry parameter, it defines the number of variables randomly sampled as candidates at each split. 

# Analysis Steps
0. Download packages & Data
1. Data set Exploration
2. Data set partition
3. SVM models:
    3.1 Unbalanced Data
    3.1.Balanced Data
4. Random Fores
    4.1.Unbalanced Data
    4.1.Balanced Data

    
# Results  
### 0. Download packages & Data needed
```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
##0.Dataset and Packages downloading 
    ##Packages Download
    if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
    if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
    if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
    
    library(tidyverse)
    library(caret)
    library(lattice)
    library(ggplot2)
    library(data.table)
    library(dplyr)
    library("readr")

    # Online Shoppers Purchasing Intention Dataset, csv downloaded via:
    #http://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset
    #http://archive.ics.uci.edu/ml/machine-learning-databases/00468/
  
    #Creating data-set:
    data<-read.csv("online_shoppers_intention (1).csv")
  
    ```
### 1. Dataset Exploration
Here we can see:
1. The data structure:
```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
str(data)
```
2. The most relevant statistics:
```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
summary(data)
```
3. The number of completed purchases (TRUE) and the number of not completed purchased (FALSE)
```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
table(data$Revenue)
```
  As seen in the basic explorarion of the dataset, there is no missing values and  modification of the Revenue feature class is needed:

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
  #From logical to chr : Revenue
  data <- data %>% mutate(Revenue = replace(Revenue, Revenue == "FALSE","KO"))
  data <- data %>% mutate(Revenue = replace(Revenue, Revenue == "TRUE","GOOD"))
  class(data$Revenue)

```

### 2. Dataset partition: Trainig & Test + Blanced Dataset
First we partition the data set: 85% Training, 15% test.
Secondly we prapared the balanced data set with the downsampling method.
```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
##2.1Creating training-test dataset
  n=nrow(data)
  ind=1:n
  itraining=sample(ind,floor(n*0.85))
  itest=sample(setdiff(ind,itraining),floor(n*0.15))
  training = data[itraining,]
  testing = data[itest,]
  dim(training)
  dim(testing)
  
##2.2 Creating balanced dataset: Downsampling method
    downSampled_training = downSample(x=training[, -ncol(training)],
                             y=as.factor(training$Revenue))
    downSampled_testing = downSample(x=testing[, -ncol(testing)],
                                      y=as.factor(testing$Revenue))
```

### 3. SVM Model
Let's start applying our SVM model. Firstly with unbalanced data, secondly with balanced data.
The predictors features included in our model are the folowing:
```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
  predictors = names(training)[names(training) != "Revenue"]
  predictors 
```

#### 3.1 SVM Linear  - Unbalanced Model
```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
  
##3.1. Unbalanced Data: Training svm unbalanced data. Fiting the model-> C parameter
    train_control<-trainControl(method="cv", number = 10, p = .9)
    svm_fit_u_l <- train(Revenue ~ ., method = "svmLinear", data = training, 
                     trControl = train_control, 
                     tuneGrid = expand.grid(C = seq(0.01, 0.2, length = 10)))
```

In this plot we can see how well each C parameter performs:
```{r c_p, echo = FALSE, fig.height=4, fig.width=5}
c_p<- plot(svm_fit_u_l)
c_p
```

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
 # The best tuning parameter C that maximizes model accuracy
    c<-svm_fit_u_l$bestTune
    results_acc_by_c_u_l<-as_tibble(svm_fit_u_l$results[which.max(svm_fit_u_l$results[,2]),])
    results_acc_by_c_u_l
    
    # Applying best C
    svm_def_u_l <- train(Revenue ~ ., method = "svmLinear", data = training, 
                     trControl = train_control, 
                     cost= c)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
       
    # LinearModel Accuracy for Unbalanced Data
    y_hat_svm <- predict(svm_def_u_l, testing, type = "raw")
    confusion_matrix_u_l<-confusionMatrix(y_hat_svm, as.factor(testing$Revenue))
    overall_Ac_u_l<-confusion_matrix_u_l$overall[["Accuracy"]]
    Sensitivity_Ac_u_l<-confusion_matrix_u_l$byClass[["Sensitivity"]]
    Specificity_Ac_u_l <-confusion_matrix_u_l$byClass[["Specificity"]]
    Pos_pred_value_u_l<-confusion_matrix_u_l$byClass[["Pos Pred Value"]]
    Neg_pred_value_u_l<-confusion_matrix_u_l$byClass[["Neg Pred Value"]]
    f_measure_u_l <- 2 * ((Pos_pred_value_u_l * Specificity_Ac_u_l) / (Pos_pred_value_u_l+ Specificity_Ac_u_l))
     cat(sprintf("Overall Accuracy=%.2f , Sensitivity = %.3f,  Specificity = %.3f,F = %.3f, PPV = %.3f\n, NPV = %.3f\n",
              overall_Ac_u_l, Sensitivity_Ac_u_l, Specificity_Ac_u_l,f_measure_u_l,Pos_pred_value_u_l,Neg_pred_value_u_l))
     print(confusion_matrix_u_l$table)
    
```

As we can see the accuracy of the balanced SVM linear model is  high, but the Sensitivity is very low.
The specifity, of course, is very high taking into account that we are dealing with unbalanced data. As we can see the PPV is,of course lower than the NPV.

#### 3.2 SVM Linear  - Balanced Model
```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
  
    ##3.2. Balanced Data:SVM
    
      ##Training svm balanced data. Fiting the model-> C parameter
      svm_fit_b_l <- train(Class ~ ., method = "svmLinear", data = downSampled_training, 
                       trControl = train_control, 
                       tuneGrid = expand.grid(C = seq(0.01, 0.2, length = 10)))
```

In this plot we can see how well each C parameter performs:
```{r c_p2, echo = FALSE, fig.height=4, fig.width=5}
c_p2<- plot(svm_fit_b_l)
c_p2
```

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
  # The best tuning parameter C that maximizes model accuracy
      cbl<-svm_fit_b_l$bestTune
      results_acc_by_c_b_l<-as_tibble(svm_fit_b_l$results[which.max(svm_fit_b_l$results[,2]),])
      results_acc_by_c_b_l
      
  # Aplying best C
      svm_def_b_l <- train(Class ~ ., method = "svmLinear", data = downSampled_training, 
                       trControl = train_control, cost=cbl)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
       
 # LinearModel Accuracy for Balanced Data
      y_hat_svm_b_l <- predict(svm_def_b_l, downSampled_testing, type = "raw")
      Confusion_Matrix_b_l<-confusionMatrix(y_hat_svm_b_l, as.factor(downSampled_testing$Class))
      Overall_Ac_b_l<-Confusion_Matrix_b_l$overall[["Accuracy"]]
      Sensitivity_Ac_b_l<-Confusion_Matrix_b_l$byClass[["Sensitivity"]]
      Specificity_Ac_b_l <-Confusion_Matrix_b_l$byClass[["Specificity"]]
      Pos_pred_value_b_l<-Confusion_Matrix_b_l$byClass[["Pos Pred Value"]]
      Neg_pred_value_b_l<-Confusion_Matrix_b_l$byClass[["Neg Pred Value"]]
      f_measure_b_l <- 2 * ((Pos_pred_value_b_l * Specificity_Ac_b_l) / (Pos_pred_value_b_l+ Specificity_Ac_b_l))
      cat(sprintf("Overall Accuracy=%.2f , Sensitivity = %.3f,  Specificity = %.3f,F = %.3f, PPV = %.3f\n, NPV = %.3f\n",
                  Overall_Ac_b_l, Sensitivity_Ac_b_l, Specificity_Ac_b_l,f_measure_b_l,Pos_pred_value_b_l,Neg_pred_value_b_l))
      print(Confusion_Matrix_b_l$table)
      
    
```
Comparing to the unbalanced model, we can see how the overall accuracy has decrease, but the sensitivity has been balanced. In this case, the PPV is greater than the NPV.

### 4. RF Model
Now let's take a look into the RF results.

#### 4.1 Random Forest  - Unbalanced Model
```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
      ##3.2 Random Forest
      ##3.2.1. Training RF unbalanced data. 
      train_control<-trainControl(method="cv", number = 10, p = .9)
      svm_fit_u_r <- train(Revenue ~ ., method = "rf", data = training, tuneLength=5,
                       trControl = train_control)
```
In this plot we can see how well each mtry parameter performs:
```{r mt, echo = FALSE, fig.height=4, fig.width=5}
mt<- plot(svm_fit_u_r)
mt
```

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
     # The best tuning mtry that maximizes model accuracy
      mtry_u<-svm_fit_u_r$bestTune
      results_acc_mtry_u_r<-as_tibble(svm_fit_u_r$results[which.max(svm_fit_u_r$results[,2]),])
      results_acc_mtry_u_r
      
      # Applying best mtry
      svm_def_u_r <- train(Revenue ~ ., method = "rf", data = training,   minNode=mtry_u$mtry,trControl = train_control)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
      # RF Accuracy for unbalanced data
      y_hat_svm_u_r <- predict(svm_def_u_r, testing, type = "raw")
      confusion_matrix_u_r<-confusionMatrix(y_hat_svm_u_r, as.factor(testing$Revenue))
      Overall_Ac_u_r<-confusion_matrix_u_r$overall[["Accuracy"]]
      Sensitivity_Ac_u_r<-confusion_matrix_u_r$byClass[["Sensitivity"]]
      Specificity_Ac_u_r <-confusion_matrix_u_r$byClass[["Specificity"]]
      Pos_pred_value_u_r<-confusion_matrix_u_r$byClass[["Pos Pred Value"]]
      Neg_pred_value_u_r<-confusion_matrix_u_r$byClass[["Neg Pred Value"]]
      f_measure_u_r <- 2 * ((Pos_pred_value_u_r * Specificity_Ac_u_r) / (Pos_pred_value_u_r + Specificity_Ac_u_r))
      cat(sprintf("Overall Accuracy=%.2f , Sensitivity = %.3f,  Specificity = %.3f,F = %.3f, PPV = %.3f\n, NPV = %.3f\n",
                  Overall_Ac_u_r, Sensitivity_Ac_u_r, Specificity_Ac_u_r,f_measure_u_r,Pos_pred_value_u_r,Neg_pred_value_u_r))
      confusion_matrix_u_r$table
```
As we can see we have increase the Overal Accuracy comparing to the SVM unbalanced. Regarding Sensitivity , the unbalanced impact is not that significant compared to the SVM model.
We are dealing again with unbalanced data so, as in the SVM model, the PPV is,of course lower than the NPV.

#### 4.2 Random Forest  - Balanced Model
```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
 ##3.2. Training RF balanced data.
      
      ##Training svm balanced data. Fiting the model-> C parameter
      svm_fit_b_r <- train(Class ~ ., method = "rf", data = downSampled_training, 
                           tuneLength=5,trControl = train_control)
```
In this plot we can see how well each mtry parameter performs:
```{r mt2, echo = FALSE, fig.height=4, fig.width=5}
mt2<-plot(svm_fit_b_r)
mt2
```

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
      # The best tuning mtry that maximizes model accuracy
      cbr<-svm_fit_b_r$bestTune
      results_acc_by_c_rb<-as_tibble(svm_fit_b_r$results[which.max(svm_fit_b_r$results[,2]),])
      results_acc_by_c_rb
      
      # Aplying best mtry
      svm_def_b_r <- train(Class ~ ., method = "rf", data = downSampled_training, 
                           minNode=cbr$mtry,
                           trControl = train_control,)
``` 


```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
      #RF Accuracy for balanced data
      y_hat_svm_b_r <- predict(svm_def_b_r, downSampled_testing, type = "raw")
      Confusion_Matrix_b_r<-confusionMatrix(y_hat_svm_b_r, as.factor(downSampled_testing$Class))
      Overall_Ac_b_r<-Confusion_Matrix_b_r$overall[["Accuracy"]]
      Sensitivity_Ac_b_r<-Confusion_Matrix_b_r$byClass[["Sensitivity"]]
      Specificity_Ac_b_r <-Confusion_Matrix_b_r$byClass[["Specificity"]]
      Pos_pred_value_b_r<-Confusion_Matrix_b_r$byClass[["Pos Pred Value"]]
      Neg_pred_value_b_r<-Confusion_Matrix_b_r$byClass[["Neg Pred Value"]]
      f_measure_b_r <- 2 * ((Pos_pred_value_b_r * Specificity_Ac_b_r) / (Pos_pred_value_b_r + Specificity_Ac_b_r))
      cat(sprintf("Overall Accuracy=%.2f , Sensitivity = %.3f,  Specificity = %.3f,F = %.3f, PPV = %.3f\n, NPV = %.3f\n",
                  Overall_Ac_b_r, Sensitivity_Ac_b_r, Specificity_Ac_b_r,f_measure_b_r,Pos_pred_value_b_r,Neg_pred_value_b_r))
      print(Confusion_Matrix_b_r$table)
``` 
The last model, presents higher level of Accuracy than the SVM balanced data and good balanced values of NPV and PPV.

# Conclusion & Discusion

The following table presents all the relevant parameters of the four applied models
```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
##Results   
    ##Results Overall Acuracy
        models_gen_accuracy <- data_frame(Model=c("SVM-Unbalanced","SVM-Balanced","RF-Unbalanced","RF-Balanced"), 
                                     Accuracy = c(overall_Ac_u_l,Overall_Ac_b_l,Overall_Ac_u_r,Overall_Ac_b_r))
       
    
    ##Results  Specificity
        models_spec <- data_frame(Model=c("SVM-Unbalanced","SVM-Balanced","RF-Unbalanced","RF-Balanced"), 
                                          Specificity = c(Specificity_Ac_u_l,Specificity_Ac_b_l,Specificity_Ac_u_r,Specificity_Ac_b_r))
    
        
    ##Results  Sensitivity
        models_sensitivity <- data_frame(Model=c("SVM-Unbalanced","SVM-Balanced","RF-Unbalanced","RF-Balanced"), 
                                         Sensitivity = c(Sensitivity_Ac_u_l,Sensitivity_Ac_b_l,Sensitivity_Ac_u_r,Sensitivity_Ac_b_r))
  
        
    ##Results TP
        models_tp <- data_frame(Model=c("SVM-Unbalanced","SVM-Balanced","RF-Unbalanced","RF-Balanced"), 
                                         TPrate= c(Pos_pred_value_u_l,Pos_pred_value_b_l,Pos_pred_value_u_r,Pos_pred_value_b_r))
 
    ##Results TN
        models_tn <- data_frame(Model=c("SVM-Unbalanced","SVM-Balanced","RF-Unbalanced","RF-Balanced"), 
                                         TNrate= c(Neg_pred_value_u_l,Neg_pred_value_b_l,Neg_pred_value_u_r,Neg_pred_value_b_r))

    ##Results F
        f_measure <- data_frame(Model=c("SVM-Unbalanced","SVM-Balanced","RF-Unbalanced","RF-Balanced"), 
                                F_measure= c(f_measure_u_l,f_measure_b_l,f_measure_u_r,f_measure_b_r))
   
    
cbind(models_gen_accuracy,models_sensitivity[ , 2:2],models_spec[ , 2:2],models_tn[ , 2:2],models_tp[ , 2:2],f_measure[ , 2:2])%>%knitr::kable()   
``` 


As we can see, the model with the highest accuracy is the RF with the original database (unbalanced) with a 90.5% accuracy.
On the other hand, since it is an unbalanced database, it presents low sensitivity ratio and a low PPV ratio.
The RF model with balanced bbdd has an acceptable accuracy (82%) and with very similar PPV and NPV ratios.
Thus we can conclude that: RF in general has a better performance than SVM in both balanced and unbalanced databases.


# Thanks for reviewing my work.
Adelaida Fernández
